{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4d4df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-Party Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    LayerNormalization,\n",
    "    LSTM,\n",
    "    MultiHeadAttention\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LABEL MAPPING SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class LabelMapper:\n",
    "    \"\"\"Advanced label mapping system for consolidating user groups\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_mapping = {}\n",
    "        self.reverse_mapping = {}\n",
    "        \n",
    "    def create_mapping(self, mapping_dict):\n",
    "        \"\"\"Create mapping from original labels to consolidated labels\"\"\"\n",
    "        self.label_mapping = {}\n",
    "        self.reverse_mapping = mapping_dict.copy()\n",
    "        \n",
    "        for consolidated_label, original_labels in mapping_dict.items():\n",
    "            for original_label in original_labels:\n",
    "                self.label_mapping[original_label] = consolidated_label\n",
    "        \n",
    "        print(\"üìã LABEL MAPPING CREATED\")\n",
    "        print(\"=\" * 30)\n",
    "        for consolidated, originals in mapping_dict.items():\n",
    "            print(f\"üè∑Ô∏è {consolidated}:\")\n",
    "            for original in originals:\n",
    "                print(f\"   ‚Ä¢ {original}\")\n",
    "        \n",
    "        return self.label_mapping\n",
    "    \n",
    "    def map_labels(self, original_labels):\n",
    "        \"\"\"Map list of original labels to consolidated labels\"\"\"\n",
    "        mapped_labels = []\n",
    "        unmapped_count = 0\n",
    "        \n",
    "        for label in original_labels:\n",
    "            if label in self.label_mapping:\n",
    "                mapped_labels.append(self.label_mapping[label])\n",
    "            else:\n",
    "                mapped_labels.append(label)\n",
    "                unmapped_count += 1\n",
    "        \n",
    "        if unmapped_count > 0:\n",
    "            print(f\"‚ö†Ô∏è Warning: {unmapped_count} labels were not mapped\")\n",
    "        \n",
    "        return mapped_labels\n",
    "    \n",
    "    def analyze_distribution(self, original_labels, mapped_labels):\n",
    "        \"\"\"Analyze label distribution before and after mapping\"\"\"\n",
    "        original_counts = Counter(original_labels)\n",
    "        mapped_counts = Counter(mapped_labels)\n",
    "        \n",
    "        print(\"\\nüìä LABEL DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"\\nüîç Original distribution:\")\n",
    "        total = len(original_labels)\n",
    "        for label, count in sorted(original_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"  ‚Ä¢ {label}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nüéØ Consolidated distribution:\")\n",
    "        for label, count in sorted(mapped_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"  ‚Ä¢ {label}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüìà Summary:\")\n",
    "        print(f\"  ‚Ä¢ Classes: {len(original_counts)} ‚Üí {len(mapped_counts)}\")\n",
    "        print(f\"  ‚Ä¢ Reduction: {len(original_counts) - len(mapped_counts)} classes\")\n",
    "        \n",
    "        return original_counts, mapped_counts\n",
    "\n",
    "# Predefined label mappings\n",
    "SUGGESTED_MAPPINGS = {\n",
    "    'human_vs_bot': {\n",
    "        'human': [\n",
    "            'tax1_users', 'tax2_users', 'profilic_survey_users',\n",
    "            'survey_desktop', 'hlisa_traces', 'za_proxy'\n",
    "        ],\n",
    "        'bot': [\n",
    "            'browser_use_agents', 'skyvern_agents', 'random_mouse_with_sleep_bot',\n",
    "            'random_mouse_bot', 'gremlins'\n",
    "        ]\n",
    "    },\n",
    "    'granular': {\n",
    "        'human': ['tax1_users', 'tax2_users', 'profilic_survey_users', 'survey_desktop'],\n",
    "        'hlisa_traces': ['hlisa_traces'],\n",
    "        'za_proxy': ['za_proxy'],\n",
    "        'browser_use_agents': ['browser_use_agents'],\n",
    "        'skyvern_agents': ['skyvern_agents'],\n",
    "        'gremlins': ['gremlins'],\n",
    "        'random_bots': ['random_mouse_with_sleep_bot', 'random_mouse_bot']\n",
    "    },\n",
    "    'default': {\n",
    "        'tax1_users': ['tax1_users'],\n",
    "        'tax2_users': ['tax2_users'],\n",
    "        'profilic_survey_users': ['profilic_survey_users'],\n",
    "        'survey_desktop': ['survey_desktop'],\n",
    "        'hlisa_traces': ['hlisa_traces'],\n",
    "        'za_proxy': ['za_proxy'],\n",
    "        'browser_use_agents': ['browser_use_agents'],\n",
    "        'skyvern_agents': ['skyvern_agents'],\n",
    "        'gremlins': ['gremlins'],\n",
    "        'random_bots': ['random_mouse_with_sleep_bot', 'random_mouse_bot']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Label Mapping System Ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA AUGMENTATION AND BALANCING SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class InteractionDataAugmenter:\n",
    "    \"\"\"Advanced data augmentation for user interaction sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed=42):\n",
    "        self.random_seed = random_seed\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def temporal_jitter(self, subsession, jitter_std=0.1):\n",
    "        \"\"\"Add small random variations to timing while preserving order\"\"\"\n",
    "        augmented = deepcopy(subsession)\n",
    "        \n",
    "        if len(augmented) < 2:\n",
    "            return augmented\n",
    "        \n",
    "        base_times = [event['timestamp_relative'] for event in augmented]\n",
    "        max_time = max(base_times) if base_times else 1.0\n",
    "        \n",
    "        for i, event in enumerate(augmented):\n",
    "            if i > 0:\n",
    "                jitter = np.random.normal(0, jitter_std * max_time)\n",
    "                min_time = augmented[i-1]['timestamp_relative'] + 0.001\n",
    "                event['timestamp_relative'] = max(min_time, event['timestamp_relative'] + jitter)\n",
    "        \n",
    "        # Recalculate time_since_last\n",
    "        for i in range(1, len(augmented)):\n",
    "            augmented[i]['time_since_last'] = (\n",
    "                augmented[i]['timestamp_relative'] - augmented[i-1]['timestamp_relative']\n",
    "            )\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def spatial_jitter(self, subsession, jitter_std=10):\n",
    "        \"\"\"Add small random variations to mouse positions\"\"\"\n",
    "        augmented = deepcopy(subsession)\n",
    "        \n",
    "        for event in augmented:\n",
    "            if event['x_pos'] > 0 or event['y_pos'] > 0:\n",
    "                event['x_pos'] += np.random.normal(0, jitter_std)\n",
    "                event['y_pos'] += np.random.normal(0, jitter_std)\n",
    "                event['x_pos'] = max(0, event['x_pos'])\n",
    "                event['y_pos'] = max(0, event['y_pos'])\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def event_type_substitution(self, subsession, substitution_prob=0.1):\n",
    "        \"\"\"Randomly substitute similar event types\"\"\"\n",
    "        similar_events = {\n",
    "            'click': ['mousedown', 'mouseup'],\n",
    "            'mousedown': ['click'],\n",
    "            'mouseup': ['click'],\n",
    "            'mousemove': ['mouseover', 'mouseout'],\n",
    "            'mouseover': ['mousemove'],\n",
    "            'mouseout': ['mousemove'],\n",
    "            'keydown': ['keyup', 'keypress'],\n",
    "            'keyup': ['keydown', 'keypress'],\n",
    "            'keypress': ['keydown', 'keyup']\n",
    "        }\n",
    "        \n",
    "        augmented = deepcopy(subsession)\n",
    "        \n",
    "        for event in augmented:\n",
    "            if random.random() < substitution_prob:\n",
    "                current_type = event['event_type']\n",
    "                if current_type in similar_events:\n",
    "                    possible_substitutes = similar_events[current_type]\n",
    "                    if possible_substitutes:\n",
    "                        event['event_type'] = random.choice(possible_substitutes)\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def sequence_cropping(self, subsession, crop_ratio_range=(0.7, 0.9)):\n",
    "        \"\"\"Randomly crop sequence while maintaining temporal structure\"\"\"\n",
    "        if len(subsession) < 3:\n",
    "            return subsession\n",
    "        \n",
    "        crop_ratio = random.uniform(*crop_ratio_range)\n",
    "        new_length = max(2, int(len(subsession) * crop_ratio))\n",
    "        start_idx = random.randint(0, len(subsession) - new_length)\n",
    "        cropped = subsession[start_idx:start_idx + new_length]\n",
    "        \n",
    "        if cropped:\n",
    "            start_time = cropped[0]['timestamp_relative']\n",
    "            for event in cropped:\n",
    "                event['timestamp_relative'] -= start_time\n",
    "        \n",
    "        return cropped\n",
    "    \n",
    "    def speed_variation(self, subsession, speed_factor_range=(0.8, 1.2)):\n",
    "        \"\"\"Vary overall speed of interactions\"\"\"\n",
    "        augmented = deepcopy(subsession)\n",
    "        speed_factor = random.uniform(*speed_factor_range)\n",
    "        \n",
    "        for event in augmented:\n",
    "            event['timestamp_relative'] *= speed_factor\n",
    "            event['time_since_last'] *= speed_factor\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def generate_synthetic_subsession(self, reference_subsessions, augmentation_strength=0.3):\n",
    "        \"\"\"Generate synthetic subsession using multiple augmentation techniques\"\"\"\n",
    "        if not reference_subsessions:\n",
    "            return None\n",
    "        \n",
    "        base_subsession = random.choice(reference_subsessions)\n",
    "        synthetic = deepcopy(base_subsession)\n",
    "        \n",
    "        # Apply augmentations with probability based on strength\n",
    "        if random.random() < augmentation_strength:\n",
    "            synthetic = self.temporal_jitter(synthetic, jitter_std=0.05 * augmentation_strength)\n",
    "        \n",
    "        if random.random() < augmentation_strength:\n",
    "            synthetic = self.spatial_jitter(synthetic, jitter_std=5 * augmentation_strength)\n",
    "        \n",
    "        if random.random() < augmentation_strength * 0.5:\n",
    "            synthetic = self.event_type_substitution(synthetic, substitution_prob=0.05 * augmentation_strength)\n",
    "        \n",
    "        if random.random() < augmentation_strength * 0.7:\n",
    "            synthetic = self.sequence_cropping(synthetic)\n",
    "        \n",
    "        if random.random() < augmentation_strength:\n",
    "            synthetic = self.speed_variation(synthetic)\n",
    "        \n",
    "        return synthetic\n",
    "\n",
    "class DataBalancer:\n",
    "    \"\"\"Balance class distribution by generating synthetic samples\"\"\"\n",
    "    \n",
    "    def __init__(self, target_balance_strategy='oversample_to_max', \n",
    "                 max_synthetic_ratio=2.0, augmentation_strength=0.3):\n",
    "        self.target_balance_strategy = target_balance_strategy\n",
    "        self.max_synthetic_ratio = max_synthetic_ratio\n",
    "        self.augmentation_strength = augmentation_strength\n",
    "        self.augmenter = InteractionDataAugmenter()\n",
    "    \n",
    "    def analyze_class_distribution(self, subsessions, labels):\n",
    "        \"\"\"Analyze current class distribution\"\"\"\n",
    "        label_counts = Counter(labels)\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        print(\"üìä CLASS DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total samples: {total_samples:,}\")\n",
    "        print(f\"Number of classes: {len(label_counts)}\")\n",
    "        \n",
    "        sorted_classes = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for label, count in sorted_classes:\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"  ‚Ä¢ {label}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        max_count = max(label_counts.values())\n",
    "        min_count = min(label_counts.values())\n",
    "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "        return label_counts, imbalance_ratio\n",
    "    \n",
    "    def calculate_target_counts(self, label_counts):\n",
    "        \"\"\"Calculate target sample counts for each class\"\"\"\n",
    "        if self.target_balance_strategy == 'oversample_to_max':\n",
    "            target_count = max(label_counts.values())\n",
    "        elif self.target_balance_strategy == 'oversample_to_mean':\n",
    "            target_count = int(np.mean(list(label_counts.values())))\n",
    "        else:\n",
    "            target_count = int(np.median(list(label_counts.values())))\n",
    "        \n",
    "        target_counts = {}\n",
    "        for label, current_count in label_counts.items():\n",
    "            max_allowed = int(current_count * (1 + self.max_synthetic_ratio))\n",
    "            target_counts[label] = min(target_count, max_allowed)\n",
    "        \n",
    "        return target_counts\n",
    "    \n",
    "    def balance_data(self, subsessions, labels, max_size_after_balancing=20000):\n",
    "        \"\"\"Balance dataset by generating synthetic samples\"\"\"\n",
    "        print(\"\\nüéØ BALANCING DATASET\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        label_counts, imbalance_ratio = self.analyze_class_distribution(subsessions, labels)\n",
    "        \n",
    "        if imbalance_ratio < 2.0:\n",
    "            print(\"‚úì Dataset is already reasonably balanced\")\n",
    "            return subsessions, labels\n",
    "        \n",
    "        target_counts = self.calculate_target_counts(label_counts)\n",
    "        \n",
    "        # Group subsessions by label\n",
    "        subsessions_by_label = {}\n",
    "        for subsession, label in zip(subsessions, labels):\n",
    "            if label not in subsessions_by_label:\n",
    "                subsessions_by_label[label] = []\n",
    "            subsessions_by_label[label].append(subsession)\n",
    "        \n",
    "        # Generate synthetic samples\n",
    "        balanced_subsessions = []\n",
    "        balanced_labels = []\n",
    "        total_synthetic = 0\n",
    "        \n",
    "        for label, target_count in target_counts.items():\n",
    "            current_subsessions = subsessions_by_label[label]\n",
    "            current_count = len(current_subsessions)\n",
    "            target_count = min(target_count, max_size_after_balancing)\n",
    "            needed = target_count - current_count\n",
    "            \n",
    "            # Add original samples\n",
    "\n",
    "            if current_count > max_size_after_balancing:\n",
    "                balanced_subsessions.extend(current_subsessions[:max_size_after_balancing])\n",
    "                balanced_labels.extend([label] * max_size_after_balancing)\n",
    "            else:\n",
    "                balanced_subsessions.extend(current_subsessions)\n",
    "                balanced_labels.extend([label] * current_count)\n",
    "            \n",
    "            # Generate synthetic samples if needed\n",
    "            if needed > 0:\n",
    "                print(f\"Generating {needed:,} synthetic samples for {label}...\")\n",
    "                \n",
    "                for _ in range(needed):\n",
    "                    synthetic_subsession = self.augmenter.generate_synthetic_subsession(\n",
    "                        current_subsessions, self.augmentation_strength\n",
    "                    )\n",
    "                    \n",
    "                    if synthetic_subsession:\n",
    "                        balanced_subsessions.append(synthetic_subsession)\n",
    "                        balanced_labels.append(label)\n",
    "                        total_synthetic += 1\n",
    "        \n",
    "        print(f\"\\n‚úÖ Balancing completed!\")\n",
    "        print(f\"  ‚Ä¢ Original: {len(subsessions):,}\")\n",
    "        print(f\"  ‚Ä¢ Synthetic: {total_synthetic:,}\")\n",
    "        print(f\"  ‚Ä¢ Total: {len(balanced_subsessions):,}\")\n",
    "        \n",
    "        # Shuffle the balanced dataset\n",
    "        combined = list(zip(balanced_subsessions, balanced_labels))\n",
    "        random.shuffle(combined)\n",
    "        balanced_subsessions, balanced_labels = zip(*combined)\n",
    "        \n",
    "        return list(balanced_subsessions), list(balanced_labels)\n",
    "\n",
    "print(\"‚úÖ Data Augmentation and Balancing System Ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRAINING MONITORING AND DIAGNOSTICS\n",
    "# ============================================================================\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    \"\"\"Advanced training monitor with issue detection\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type=\"Model\"):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.batch_losses = []\n",
    "        self.epoch_metrics = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_losses.append(logs.get('loss', 0))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'accuracy': logs.get('accuracy', 0),\n",
    "            'val_accuracy': logs.get('val_accuracy', 0)\n",
    "        }\n",
    "        self.epoch_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"\\n{self.model_type} Epoch {epoch + 1}:\")\n",
    "        print(f\"  Loss: {metrics['loss']:.4f} ‚Üí Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(f\"  Acc: {metrics['accuracy']:.4f} ‚Üí Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "        \n",
    "        # Issue detection\n",
    "        if epoch > 5:\n",
    "            recent_losses = self.batch_losses[-100:]\n",
    "            if len(recent_losses) > 10:\n",
    "                loss_std = np.std(recent_losses)\n",
    "                if loss_std < 0.001:\n",
    "                    print(\"  ‚ö†Ô∏è Loss plateau detected - consider reducing learning rate\")\n",
    "                elif loss_std > 1.0:\n",
    "                    print(\"  ‚ö†Ô∏è Loss instability - learning rate might be too high\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if epoch > 3:\n",
    "            recent_val_acc = [m['val_accuracy'] for m in self.epoch_metrics[-3:]]\n",
    "            if all(acc <= recent_val_acc[0] + 0.001 for acc in recent_val_acc):\n",
    "                print(\"  üìä No improvement in validation accuracy for 3 epochs\")\n",
    "\n",
    "def diagnose_model_issues(model, X_sample, y_sample, model_type=\"Model\"):\n",
    "    \"\"\"Comprehensive model diagnostics\"\"\"\n",
    "    print(f\"üîç DIAGNOSING {model_type.upper()} ISSUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Input data analysis\n",
    "    print(\"\\nüìä Input Data Analysis:\")\n",
    "    print(f\"  ‚Ä¢ Shape: {X_sample.shape}\")\n",
    "    print(f\"  ‚Ä¢ Range: [{X_sample.min():.3f}, {X_sample.max():.3f}]\")\n",
    "    print(f\"  ‚Ä¢ Mean: {X_sample.mean():.3f}, Std: {X_sample.std():.3f}\")\n",
    "    \n",
    "    if X_sample.std() > 100:\n",
    "        print(\"  ‚ö†Ô∏è Large input variance - consider normalization\")\n",
    "    \n",
    "    # Model architecture\n",
    "    print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
    "    total_params = model.count_params()\n",
    "    print(f\"  ‚Ä¢ Parameters: {total_params:,}\")\n",
    "    \n",
    "    if total_params > 1000000:\n",
    "        print(\"  ‚ö†Ô∏è Large model - risk of overfitting\")\n",
    "    elif total_params < 10000:\n",
    "        print(\"  ‚ö†Ô∏è Small model - risk of underfitting\")\n",
    "    \n",
    "    # Forward pass test\n",
    "    print(f\"\\nüîÑ Forward Pass Test:\")\n",
    "    try:\n",
    "        predictions = model(X_sample[:1])\n",
    "        print(f\"  ‚Ä¢ Output shape: {predictions.shape}\")\n",
    "        print(f\"  ‚Ä¢ Output range: [{predictions.numpy().min():.3f}, {predictions.numpy().max():.3f}]\")\n",
    "        \n",
    "        if tf.math.reduce_any(tf.math.is_nan(predictions)):\n",
    "            print(\"  ‚ùå NaN outputs detected!\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Forward pass successful\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Forward pass failed: {e}\")\n",
    "    \n",
    "    # Gradient flow test\n",
    "    print(f\"\\nüìà Gradient Flow Test:\")\n",
    "    try:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_sample[:32])\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y_sample[:32], predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        grad_norms = [tf.norm(grad).numpy() for grad in gradients if grad is not None]\n",
    "        \n",
    "        if grad_norms:\n",
    "            print(f\"  ‚Ä¢ Gradient norms: min={min(grad_norms):.6f}, max={max(grad_norms):.6f}\")\n",
    "            \n",
    "            if max(grad_norms) < 1e-6:\n",
    "                print(\"  ‚ö†Ô∏è Vanishing gradients detected\")\n",
    "            elif max(grad_norms) > 10:\n",
    "                print(\"  ‚ö†Ô∏è Exploding gradients detected\")\n",
    "            else:\n",
    "                print(\"  ‚úÖ Gradient flow looks healthy\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Gradient computation failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Training Monitoring and Diagnostics Ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BASE PIPELINE CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class BasePipeline:\n",
    "    \"\"\"Base class with common functionality for user classification pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self, session_gap_minutes=5, subsession_duration_seconds=30, \n",
    "                 min_events_per_subsession=10, max_sequence_length=100):\n",
    "        self.session_gap_minutes = session_gap_minutes\n",
    "        self.subsession_duration_seconds = subsession_duration_seconds\n",
    "        self.min_events_per_subsession = min_events_per_subsession\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        \n",
    "        # Encoders and scalers\n",
    "        self.event_encoder = LabelEncoder()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "        # Model and training\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_mapper = None\n",
    "        self.original_labels = None\n",
    "    \n",
    "    def segment_into_sessions(self, events_df, user_id):\n",
    "        \"\"\"Break user events into sessions based on inactivity gaps\"\"\"\n",
    "        user_events = events_df[events_df['userId'] == user_id].copy()\n",
    "        \n",
    "        if len(user_events) == 0:\n",
    "            return []\n",
    "        \n",
    "        user_events = user_events.sort_values('timestamp_num').copy()\n",
    "        user_events['time_diff_minutes'] = user_events['timestamp_num'].diff() / (1000 * 60)\n",
    "        \n",
    "        session_breaks = user_events['time_diff_minutes'] > self.session_gap_minutes\n",
    "        user_events['session_id'] = session_breaks.cumsum()\n",
    "        \n",
    "        sessions = []\n",
    "        for session_id in user_events['session_id'].unique():\n",
    "            session_events = user_events[user_events['session_id'] == session_id].copy()\n",
    "            if len(session_events) >= 2:\n",
    "                sessions.append(session_events)\n",
    "        \n",
    "        return sessions\n",
    "    \n",
    "    def create_subsessions(self, session_events):\n",
    "        \"\"\"Create fixed-duration subsessions from a session\"\"\"\n",
    "        if len(session_events) == 0:\n",
    "            return []\n",
    "        \n",
    "        session_events = session_events.sort_values('timestamp_num').copy()\n",
    "        \n",
    "        start_time = session_events['timestamp_num'].min()\n",
    "        end_time = session_events['timestamp_num'].max()\n",
    "        session_duration = (end_time - start_time) / 1000\n",
    "        \n",
    "        subsession_duration_ms = self.subsession_duration_seconds * 1000\n",
    "        num_subsessions = max(1, int(session_duration / self.subsession_duration_seconds))\n",
    "        \n",
    "        subsessions = []\n",
    "        \n",
    "        for i in range(num_subsessions):\n",
    "            subsession_start = start_time + (i * subsession_duration_ms)\n",
    "            subsession_end = subsession_start + subsession_duration_ms\n",
    "            \n",
    "            subsession_mask = (\n",
    "                (session_events['timestamp_num'] >= subsession_start) & \n",
    "                (session_events['timestamp_num'] < subsession_end)\n",
    "            )\n",
    "            \n",
    "            subsession_events = session_events[subsession_mask].copy()\n",
    "            \n",
    "            if len(subsession_events) >= self.min_events_per_subsession:\n",
    "                subsessions.append(subsession_events)\n",
    "        \n",
    "        return subsessions\n",
    "    \n",
    "    def extract_features_from_subsession(self, subsession_events):\n",
    "        \"\"\"Extract features from a subsession\"\"\"\n",
    "        if len(subsession_events) == 0:\n",
    "            return None\n",
    "        \n",
    "        subsession_events = subsession_events.sort_values('timestamp_num').copy()\n",
    "        features = []\n",
    "        \n",
    "        for idx, event in subsession_events.iterrows():\n",
    "            event_features = {\n",
    "                'event_type': event.get('eventName', 'unknown'),\n",
    "                'timestamp_relative': 0,\n",
    "                'x_pos': 0,\n",
    "                'y_pos': 0,\n",
    "                'time_since_last': 0\n",
    "            }\n",
    "            \n",
    "            # Extract position if available\n",
    "            if 'pos' in event and event['pos']:\n",
    "                try:\n",
    "                    import json\n",
    "                    pos_data = json.loads(event['pos']) if isinstance(event['pos'], str) else event['pos']\n",
    "                    event_features['x_pos'] = pos_data.get('x', 0)\n",
    "                    event_features['y_pos'] = pos_data.get('y', 0)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            features.append(event_features)\n",
    "        \n",
    "        # Calculate relative timestamps and time differences\n",
    "        start_time = subsession_events['timestamp_num'].min()\n",
    "        prev_time = start_time\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            current_time = subsession_events.iloc[i]['timestamp_num']\n",
    "            feature['timestamp_relative'] = (current_time - start_time) / 1000\n",
    "            feature['time_since_last'] = (current_time - prev_time) / 1000\n",
    "            prev_time = current_time\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_with_label_mapping(self, mixed_sample_events, label_mapping=None):\n",
    "        \"\"\"Process mixed sample events with optional label mapping\"\"\"\n",
    "        print(\"üîÑ PROCESSING EVENTS WITH LABEL MAPPING\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        all_subsessions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for label, data in mixed_sample_events.items():\n",
    "            print(f\"\\nüìä Processing {label}...\")\n",
    "            \n",
    "            events_df = data['events_df']\n",
    "            user_count = 0\n",
    "            subsession_count = 0\n",
    "            \n",
    "            for user_id in events_df['userId'].unique():\n",
    "                user_count += 1\n",
    "                sessions = self.segment_into_sessions(events_df, user_id)\n",
    "                \n",
    "                for session in sessions:\n",
    "                    subsessions = self.create_subsessions(session)\n",
    "                    \n",
    "                    for subsession in subsessions:\n",
    "                        features = self.extract_features_from_subsession(subsession)\n",
    "                        \n",
    "                        if features and len(features) >= self.min_events_per_subsession:\n",
    "                            all_subsessions.append(features)\n",
    "                            all_labels.append(label)\n",
    "                            subsession_count += 1\n",
    "            \n",
    "            print(f\"  ‚úì {user_count} users ‚Üí {subsession_count} subsessions\")\n",
    "        \n",
    "        # Apply label mapping if provided\n",
    "        if label_mapping:\n",
    "            print(f\"\\nüè∑Ô∏è APPLYING LABEL MAPPING\")\n",
    "            print(\"=\" * 30)\n",
    "            \n",
    "            mapper = LabelMapper()\n",
    "            mapper.create_mapping(label_mapping)\n",
    "            \n",
    "            original_labels = all_labels.copy()\n",
    "            mapped_labels = mapper.map_labels(all_labels)\n",
    "            \n",
    "            mapper.analyze_distribution(original_labels, mapped_labels)\n",
    "            \n",
    "            self.label_mapper = mapper\n",
    "            self.original_labels = original_labels\n",
    "            \n",
    "            return all_subsessions, mapped_labels\n",
    "        \n",
    "        return all_subsessions, all_labels\n",
    "    \n",
    "    def encode_subsessions(self, subsessions, labels):\n",
    "        \"\"\"Encode subsessions into numerical format\"\"\"\n",
    "        print(\"\\nüî¢ ENCODING SUBSESSIONS\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        # Collect all event types\n",
    "        all_event_types = set()\n",
    "        for subsession in subsessions:\n",
    "            for event in subsession:\n",
    "                all_event_types.add(event['event_type'])\n",
    "        \n",
    "        self.event_encoder.fit(list(all_event_types))\n",
    "        self.label_encoder.fit(labels)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Event types: {len(all_event_types)}\")\n",
    "        print(f\"  ‚Ä¢ Labels: {len(set(labels))}\")\n",
    "        \n",
    "        # Encode subsessions\n",
    "        encoded_sequences = []\n",
    "        \n",
    "        for subsession in subsessions:\n",
    "            limited_subsession = subsession[:self.max_sequence_length]\n",
    "            \n",
    "            sequence = []\n",
    "            for event in limited_subsession:\n",
    "                encoded_event = [\n",
    "                    self.event_encoder.transform([event['event_type']])[0],\n",
    "                    event['timestamp_relative'],\n",
    "                    event['x_pos'],\n",
    "                    event['y_pos'],\n",
    "                    event['time_since_last']\n",
    "                ]\n",
    "                sequence.append(encoded_event)\n",
    "            \n",
    "            encoded_sequences.append(sequence)\n",
    "        \n",
    "        # Pad sequences\n",
    "        max_len = min(self.max_sequence_length, max(len(seq) for seq in encoded_sequences))\n",
    "        X = np.zeros((len(encoded_sequences), max_len, 5))\n",
    "        \n",
    "        for i, sequence in enumerate(encoded_sequences):\n",
    "            seq_len = min(len(sequence), max_len)\n",
    "            X[i, :seq_len, :] = sequence[:seq_len]\n",
    "        \n",
    "        # Encode labels\n",
    "        y = self.label_encoder.transform(labels)\n",
    "        y = to_categorical(y)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Encoded shape: {X.shape}\")\n",
    "        print(f\"  ‚Ä¢ Labels shape: {y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def preprocess_features(self, X):\n",
    "        \"\"\"Preprocess features for better training\"\"\"\n",
    "        print(\"üîÑ PREPROCESSING FEATURES\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Feature-wise normalization\n",
    "        for feature_idx in range(X.shape[2]):\n",
    "            feature_data = X[:, :, feature_idx]\n",
    "            \n",
    "            if feature_idx == 0:  # Event type (categorical)\n",
    "                continue\n",
    "            else:  # Continuous features\n",
    "                non_zero_data = feature_data[feature_data != 0]\n",
    "                if len(non_zero_data) > 0:\n",
    "                    p5, p95 = np.percentile(non_zero_data, [5, 95])\n",
    "                    if p95 > p5:\n",
    "                        feature_data = np.clip(feature_data, p5, p95)\n",
    "                        feature_data = (feature_data - p5) / (p95 - p5)\n",
    "                        X_processed[:, :, feature_idx] = feature_data\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Original range: [{X.min():.3f}, {X.max():.3f}]\")\n",
    "        print(f\"  ‚Ä¢ Processed range: [{X_processed.min():.3f}, {X_processed.max():.3f}]\")\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the trained model\"\"\"\n",
    "        print(\"\\nüìä MODEL EVALUATION\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_pred_classes = y_pred.argmax(axis=1)\n",
    "        y_true_classes = self.y_test.argmax(axis=1)\n",
    "        \n",
    "        class_names = self.label_encoder.classes_\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "        print(cm)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'{self.__class__.__name__} - Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Training history\n",
    "        if self.history:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            ax1.plot(self.history.history['accuracy'], label='Training')\n",
    "            ax1.plot(self.history.history['val_accuracy'], label='Validation')\n",
    "            ax1.set_title('Model Accuracy')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Accuracy')\n",
    "            ax1.legend()\n",
    "            \n",
    "            ax2.plot(self.history.history['loss'], label='Training')\n",
    "            ax2.plot(self.history.history['val_loss'], label='Validation')\n",
    "            ax2.set_title('Model Loss')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Loss')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"‚úÖ Base Pipeline Class Ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ADVANCED LSTM PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMPipeline(BasePipeline):\n",
    "    \"\"\"Advanced LSTM pipeline with optimized architecture and training\"\"\"\n",
    "    \n",
    "    def __init__(self, session_gap_minutes=5, subsession_duration_seconds=30, \n",
    "                 min_events_per_subsession=10, max_sequence_length=100):\n",
    "        super().__init__(session_gap_minutes, subsession_duration_seconds,\n",
    "                        min_events_per_subsession, max_sequence_length)\n",
    "    \n",
    "    def build_lstm_model(self, input_shape, num_classes):\n",
    "        \"\"\"Build optimized LSTM model\"\"\"\n",
    "        print(\"\\nüèóÔ∏è BUILDING ADVANCED LSTM MODEL\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        model = Sequential([\n",
    "            # Input normalization\n",
    "            BatchNormalization(input_shape=input_shape),\n",
    "            \n",
    "            # First LSTM layer\n",
    "            LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Second LSTM layer\n",
    "            LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Dense layers with regularization\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Conservative optimizer\n",
    "        optimizer = Adam(\n",
    "            learning_rate=0.001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Input shape: {input_shape}\")\n",
    "        print(f\"  ‚Ä¢ Output classes: {num_classes}\")\n",
    "        print(f\"  ‚Ä¢ Total parameters: {model.count_params():,}\")\n",
    "        print(f\"  ‚Ä¢ Regularization: Dropout + BatchNorm + Gradient clipping\")\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, validation_split=0.2, epochs=6, batch_size=32):\n",
    "        \"\"\"Train LSTM model with enhanced monitoring\"\"\"\n",
    "        print(\"\\nüöÄ TRAINING ADVANCED LSTM MODEL\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Preprocess features\n",
    "        X_processed = self.preprocess_features(X)\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X_processed, y, test_size=validation_split, random_state=42, \n",
    "            stratify=y.argmax(axis=1)\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Training samples: {len(self.X_train):,}\")\n",
    "        print(f\"  ‚Ä¢ Testing samples: {len(self.X_test):,}\")\n",
    "        \n",
    "        # Callbacks\n",
    "        monitor = TrainingMonitor(\"LSTM\")\n",
    "        callbacks = [\n",
    "            monitor,\n",
    "            EarlyStopping(\n",
    "                patience=12,\n",
    "                restore_best_weights=True,\n",
    "                monitor='val_accuracy',\n",
    "                mode='max'\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                patience=6,\n",
    "                factor=0.5,\n",
    "                min_lr=1e-6,\n",
    "                monitor='val_accuracy',\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def run_complete_pipeline(self, mixed_sample_events, enable_balancing=True, \n",
    "                            label_mapping=None, diagnose=True):\n",
    "        \"\"\"Run complete LSTM pipeline\"\"\"\n",
    "        print(\"üöÄ RUNNING COMPLETE ADVANCED LSTM PIPELINE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Step 1: Process with label mapping\n",
    "        subsessions, labels = self.process_with_label_mapping(mixed_sample_events, label_mapping)\n",
    "        \n",
    "        if len(subsessions) == 0:\n",
    "            print(\"‚ùå No subsessions generated\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Balance data\n",
    "        if enable_balancing:\n",
    "            balancer = DataBalancer(\n",
    "                target_balance_strategy='oversample_to_max',\n",
    "                max_synthetic_ratio=2.0,\n",
    "                augmentation_strength=0.3\n",
    "            )\n",
    "            subsessions, labels = balancer.balance_data(subsessions, labels)\n",
    "        \n",
    "        # Step 3: Encode subsessions\n",
    "        X, y = self.encode_subsessions(subsessions, labels)\n",
    "        \n",
    "        # Step 4: Build model\n",
    "        input_shape = (X.shape[1], X.shape[2])\n",
    "        num_classes = y.shape[1]\n",
    "        self.build_lstm_model(input_shape, num_classes)\n",
    "        \n",
    "        # Step 5: Diagnose if requested\n",
    "        if diagnose:\n",
    "            diagnose_model_issues(self.model, X[:100], y[:100], \"LSTM\")\n",
    "        \n",
    "        # Step 6: Train model\n",
    "        self.train_model(X, y)\n",
    "        \n",
    "        # Step 7: Evaluate model\n",
    "        self.evaluate_model()\n",
    "        \n",
    "        print(\"\\n‚úÖ Advanced LSTM Pipeline completed!\")\n",
    "        return self.model\n",
    "    \n",
    "    # Convenience methods\n",
    "    def run_with_human_vs_bot(self, mixed_sample_events, enable_balancing=True):\n",
    "        \"\"\"Run with human vs bot classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['human_vs_bot']\n",
    "        )\n",
    "    \n",
    "    def run_with_granular_mapping(self, mixed_sample_events, enable_balancing=True):\n",
    "        \"\"\"Run with granular classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['granular']\n",
    "        )\n",
    "    \n",
    "    def run_with_default_mapping(self, mixed_sample_events, enable_balancing=False):\n",
    "        \"\"\"Run with default classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['default']\n",
    "        )\n",
    "    \n",
    "    def run_with_custom_mapping(self, mixed_sample_events, custom_mapping, enable_balancing=True):\n",
    "        \"\"\"Run with custom mapping\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, custom_mapping\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Advanced LSTM Pipeline Ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. PIPELINE INITIALIZATION AND UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_pipelines():\n",
    "    \"\"\"Initialize both LSTM and Transformer pipelines\"\"\"\n",
    "    print(\"üöÄ INITIALIZING ADVANCED PIPELINES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # LSTM Pipeline\n",
    "    lstm_pipeline = LSTMPipeline(\n",
    "        session_gap_minutes=5,\n",
    "        subsession_duration_seconds=0.5,\n",
    "        min_events_per_subsession=8,\n",
    "        max_sequence_length=1000\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Pipeline initialized successfully!\")\n",
    "    return lstm_pipeline\n",
    "\n",
    "def show_available_mappings():\n",
    "    \"\"\"Display all available predefined label mappings\"\"\"\n",
    "    print(\"\\nüéØ AVAILABLE LABEL MAPPINGS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    for scenario_name, mapping in SUGGESTED_MAPPINGS.items():\n",
    "        print(f\"\\nüìã {scenario_name.upper()}:\")\n",
    "        print(\"-\" * 25)\n",
    "        for consolidated_label, original_labels in mapping.items():\n",
    "            print(f\"üè∑Ô∏è {consolidated_label}:\")\n",
    "            for original in original_labels:\n",
    "                print(f\"   ‚Ä¢ {original}\")\n",
    "\n",
    "\n",
    "# Initialize pipelines\n",
    "lstm_pipeline = initialize_pipelines()\n",
    "\n",
    "print(\"\\nüéØ USAGE EXAMPLES:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "examples = '''\n",
    "# Example 1: Quick human vs bot classification with LSTM\n",
    "lstm_model = lstm_pipeline.run_with_human_vs_bot(mixed_sample_events)\n",
    "\n",
    "# Example 2: Transformer with custom mapping\n",
    "custom_mapping = {\n",
    "    'legitimate_users': ['tax1_users', 'tax2_users', 'survey_desktop'],\n",
    "    'automation_bots': ['browser_use_agents', 'skyvern_agents'],\n",
    "    'malicious_bots': ['random_mouse_bot', 'gremlins']\n",
    "}\n",
    "transformer_model = transformer_pipeline.run_with_custom_mapping(\n",
    "    mixed_sample_events, custom_mapping\n",
    ")\n",
    "\n",
    "# Example 3: Compare both models\n",
    "results = compare_models(\n",
    "    lstm_pipeline, transformer_pipeline, mixed_sample_events, \n",
    "    SUGGESTED_MAPPINGS['human_vs_bot']\n",
    ")\n",
    "\n",
    "# Example 4: Show available mappings\n",
    "show_available_mappings()\n",
    "\n",
    "# Example 5: Full pipeline with all options\n",
    "model = lstm_pipeline.run_complete_pipeline(\n",
    "    mixed_sample_events,\n",
    "    enable_balancing=True,\n",
    "    label_mapping=SUGGESTED_MAPPINGS['granular'],\n",
    "    diagnose=True\n",
    ")\n",
    "'''\n",
    "\n",
    "print(examples)\n",
    "\n",
    "print(\"üéâ ADVANCED USER CLASSIFICATION MODELS READY!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Features included:\")\n",
    "print(\"  ‚Ä¢ Optimized LSTM and Transformer architectures\")\n",
    "print(\"  ‚Ä¢ Advanced data balancing and augmentation\")\n",
    "print(\"  ‚Ä¢ Flexible label mapping system\")\n",
    "print(\"  ‚Ä¢ Comprehensive training monitoring\")\n",
    "print(\"  ‚Ä¢ Built-in diagnostics and issue detection\")\n",
    "print(\"  ‚Ä¢ Easy-to-use convenience methods\")\n",
    "print(\"  ‚Ä¢ Model comparison utilities\")\n",
    "print(\"  ‚Ä¢ Complete data loading and processing pipeline\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüöÄ Ready to classify user interactions with state-of-the-art models!\")\n",
    "print(\"\\nüìã QUICK START OPTIONS:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Train model for classification\n",
    "# lstm_model = lstm_pipeline.run_with_granular_mapping(mixed_sample_events)\n",
    "\n",
    "print(workflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaec61e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, LayerNormalization,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D, Input, Add,\n",
    "    Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class CNNPipeline(BasePipeline):\n",
    "    \"\"\"\n",
    "    Alternative 1D CNN pipeline that tries Conv1D first, falls back to Dense layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, session_gap_minutes=5, subsession_duration_seconds=1, \n",
    "                 min_events_per_subsession=10, max_sequence_length=100,\n",
    "                 num_filters=[64, 128, 256], kernel_sizes=[3, 5, 7], \n",
    "                 pool_sizes=[2, 2, 2], dropout_rate=0.3):\n",
    "        super().__init__(session_gap_minutes, subsession_duration_seconds,\n",
    "                        min_events_per_subsession, max_sequence_length)\n",
    "        \n",
    "        # CNN hyperparameters\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_conv1d = True  # Will be set to False if Conv1D fails\n",
    "    \n",
    "    def build_cnn_model(self, input_shape, num_classes):\n",
    "        \"\"\"Build 1D CNN model with fallback to dense layers\"\"\"\n",
    "        print(\"\\nüèóÔ∏è BUILDING 1D CNN MODEL WITH FALLBACK\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        try:\n",
    "            # Try to build with Conv1D layers first\n",
    "            print(\"  üîÑ Attempting Conv1D layers...\")\n",
    "            model = self._build_conv1d_model(input_shape, num_classes)\n",
    "            \n",
    "            # Test if Conv1D works by doing a small forward pass\n",
    "            test_input = tf.random.normal((1,) + input_shape)\n",
    "            _ = model(test_input)\n",
    "            \n",
    "            print(\"  ‚úÖ Conv1D layers working successfully!\")\n",
    "            self.use_conv1d = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Conv1D failed ({str(e)[:50]}...), falling back to Dense layers\")\n",
    "            model = self._build_dense_model(input_shape, num_classes)\n",
    "            self.use_conv1d = False\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def _build_conv1d_model(self, input_shape, num_classes):\n",
    "        \"\"\"Build model with Conv1D layers\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input normalization\n",
    "        model.add(BatchNormalization(input_shape=input_shape))\n",
    "        \n",
    "        # First CNN block\n",
    "        model.add(Conv1D(\n",
    "            filters=self.num_filters[0], \n",
    "            kernel_size=self.kernel_sizes[0],\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=self.pool_sizes[0]))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Second CNN block\n",
    "        model.add(Conv1D(\n",
    "            filters=self.num_filters[1],\n",
    "            kernel_size=self.kernel_sizes[1],\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=self.pool_sizes[1]))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Global pooling\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(self.dropout_rate * 0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Architecture: Conv1D layers\")\n",
    "        return model\n",
    "    \n",
    "    def _build_dense_model(self, input_shape, num_classes):\n",
    "        \"\"\"Build model with Dense layers (fallback)\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Flatten input\n",
    "        model.add(Flatten(input_shape=input_shape))\n",
    "        \n",
    "        # Dense layers simulating CNN behavior\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Architecture: Dense layers (CPU fallback)\")\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, validation_split=0.2, epochs=50, batch_size=32):\n",
    "        \"\"\"Train model with enhanced monitoring\"\"\"\n",
    "        print(f\"\\nüöÄ TRAINING {'CONV1D' if self.use_conv1d else 'DENSE'} MODEL\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Preprocess features\n",
    "        X_processed = self.preprocess_features(X)\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X_processed, y, test_size=validation_split, random_state=42, \n",
    "            stratify=y.argmax(axis=1)\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Training samples: {len(self.X_train):,}\")\n",
    "        print(f\"  ‚Ä¢ Testing samples: {len(self.X_test):,}\")\n",
    "        \n",
    "        # Callbacks\n",
    "        monitor = TrainingMonitor(\"Alternative CNN\")\n",
    "        callbacks = [\n",
    "            monitor,\n",
    "            EarlyStopping(\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                monitor='val_accuracy',\n",
    "                mode='max'\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                patience=5,\n",
    "                factor=0.5,\n",
    "                min_lr=1e-6,\n",
    "                monitor='val_accuracy',\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def run_complete_pipeline(self, mixed_sample_events, enable_balancing=True, \n",
    "                            label_mapping=None, diagnose=True):\n",
    "        \"\"\"Run complete alternative CNN pipeline\"\"\"\n",
    "        print(\"üöÄ RUNNING ALTERNATIVE 1D CNN PIPELINE\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Step 1: Process with label mapping\n",
    "        subsessions, labels = self.process_with_label_mapping(mixed_sample_events, label_mapping)\n",
    "        \n",
    "        if len(subsessions) == 0:\n",
    "            print(\"‚ùå No subsessions generated\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Balance data\n",
    "        if enable_balancing:\n",
    "            balancer = DataBalancer(\n",
    "                target_balance_strategy='oversample_to_max',\n",
    "                max_synthetic_ratio=2.0,\n",
    "                augmentation_strength=0.3\n",
    "            )\n",
    "            subsessions, labels = balancer.balance_data(subsessions, labels)\n",
    "        \n",
    "        # Step 3: Encode subsessions\n",
    "        X, y = self.encode_subsessions(subsessions, labels)\n",
    "        \n",
    "        # Step 4: Build model\n",
    "        input_shape = (X.shape[1], X.shape[2])\n",
    "        num_classes = y.shape[1]\n",
    "        self.build_cnn_model(input_shape, num_classes)\n",
    "        \n",
    "        # Step 5: Diagnose if requested\n",
    "        if diagnose:\n",
    "            diagnose_model_issues(self.model, X[:100], y[:100], \"Alternative CNN\")\n",
    "        \n",
    "        # Step 6: Train model\n",
    "        self.train_model(X, y)\n",
    "        \n",
    "        # Step 7: Evaluate model\n",
    "        self.evaluate_model()\n",
    "        \n",
    "        print(f\"\\n‚úÖ CNN Pipeline completed using {'Conv1D' if self.use_conv1d else 'Dense'} layers!\")\n",
    "        return self.model\n",
    "    \n",
    "    # Convenience methods\n",
    "    def run_with_human_vs_bot(self, mixed_sample_events, enable_balancing=True):\n",
    "        \"\"\"Run with human vs bot classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['human_vs_bot']\n",
    "        )\n",
    "\n",
    "    def run_with_granular_mapping(self, mixed_sample_events, enable_balancing=True):\n",
    "        \"\"\"Run with granular classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['granular']\n",
    "        )\n",
    "    \n",
    "    def run_with_default_mapping(self, mixed_sample_events, enable_balancing=True):\n",
    "        \"\"\"Run with default classification\"\"\"\n",
    "        return self.run_complete_pipeline(\n",
    "            mixed_sample_events, enable_balancing, SUGGESTED_MAPPINGS['default']\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ 1D CNN Pipeline Ready!\")\n",
    "\n",
    "# cnn_pipeline = CNNPipeline(\n",
    "#    session_gap_minutes=5,\n",
    "#    subsession_duration_seconds=0.5,\n",
    "#    min_events_per_subsession=10,\n",
    "#    max_sequence_length=100\n",
    "# )\n",
    "\n",
    "# Train normally - it will handle the DNN library issue automatically\n",
    "# cnn_model = cnn_pipeline.run_with_default_mapping(mixed_sample_events)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
