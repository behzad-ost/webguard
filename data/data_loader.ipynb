{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9163d7",
   "metadata": {},
   "source": [
    "## Load Data from Mongo collection and json files\n",
    "\n",
    "In this file we will combine all the datasets together.\n",
    "Variable ```mixed_sample_events``` will contain all the extracted data.\n",
    "\n",
    "\n",
    "The domain names are modified for anonymity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135df9aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "def connect_to_mongodb(connection_string=\"mongodb://mongo_server:27017/\"):\n",
    "    \"\"\"Connect to MongoDB database\"\"\"\n",
    "    try:\n",
    "        client = MongoClient(connection_string)\n",
    "        # Test the connection\n",
    "        client.admin.command('ping')\n",
    "        print(\"‚úì Connected to MongoDB successfully\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to connect to MongoDB: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_domain_from_url(url):\n",
    "    \"\"\"Extract domain from URL, handling various URL formats\"\"\"\n",
    "    try:\n",
    "        # Handle file:// URLs\n",
    "        if url.startswith('file:///'):\n",
    "            # Extract the drive/path pattern for file URLs\n",
    "            match = re.match(r'file:///([A-Za-z]:/[^/]*)', url)\n",
    "            if match:\n",
    "                return f\"file:///{match.group(1)}\"\n",
    "            return \"file:///local\"\n",
    "        \n",
    "        # Handle regular URLs\n",
    "        parsed = urlparse(url)\n",
    "        if parsed.netloc:\n",
    "            # Remove www. prefix for cleaner grouping\n",
    "            domain = parsed.netloc.lower()\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            return f\"{parsed.scheme}://{domain}\"\n",
    "        \n",
    "        # Fallback for malformed URLs\n",
    "        return url.split('/')[0] if '/' in url else url\n",
    "    \n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def load_mongodb_to_dataframe(db_name=\"test\", collection_name=\"userIds\"):\n",
    "    \"\"\"Load MongoDB collection into pandas DataFrame\"\"\"\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    client = connect_to_mongodb()\n",
    "    if not client:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Access database and collection\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        print(f\"Loading data from collection: {db_name}.{collection_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get all documents\n",
    "        documents = list(collection.find({}))\n",
    "        \n",
    "        if not documents:\n",
    "            print(\"No documents found in the collection\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Loaded {len(documents)} documents from MongoDB\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(documents)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "# Load the data\n",
    "print(\"MongoDB User Data Analysis - Pandas Version\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df = load_mongodb_to_dataframe(\"test\", \"userIds\")\n",
    "if df is not None:\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "# Expand URLs from metadata and create a detailed analysis DataFrame\n",
    "if df is not None:\n",
    "    print(\"Expanding URLs from metadata...\")\n",
    "    \n",
    "    # Create a list to store expanded data\n",
    "    expanded_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        user_id = row.get('_id', 'unknown')\n",
    "        \n",
    "        # Extract URLs from metadata\n",
    "        if 'metadata' in row and isinstance(row['metadata'], dict) and 'urls' in row['metadata']:\n",
    "            urls = row['metadata']['urls']\n",
    "            if isinstance(urls, list):\n",
    "                url_list = urls\n",
    "            elif isinstance(urls, str):\n",
    "                url_list = [urls]\n",
    "            else:\n",
    "                url_list = []\n",
    "            \n",
    "            # Add each URL as a separate row\n",
    "            for url in url_list:\n",
    "                domain = extract_domain_from_url(url)\n",
    "                \n",
    "                expanded_data.append({\n",
    "                    'user_id': user_id,\n",
    "                    'full_url': url,\n",
    "                    'domain': domain,\n",
    "                    'event_count': row['metadata'].get('eventCount', 0) if 'metadata' in row else 0,\n",
    "                    'first_timestamp': row['metadata'].get('firstTimestamp', '') if 'metadata' in row else '',\n",
    "                    'last_timestamp': row['metadata'].get('lastTimestamp', '') if 'metadata' in row else '',\n",
    "                    'user_agent': row['metadata'].get('fingerprint', {}).get('userAgent', '') if 'metadata' in row and 'fingerprint' in row['metadata'] else '',\n",
    "                    'source_ip': row['metadata'].get('fingerprint', {}).get('source', '') if 'metadata' in row and 'fingerprint' in row['metadata'] else ''\n",
    "                })\n",
    "    \n",
    "    # Create expanded DataFrame\n",
    "    url_df = pd.DataFrame(expanded_data)\n",
    "    \n",
    "    print(f\"\\nExpanded DataFrame created!\")\n",
    "    print(f\"Shape: {url_df.shape}\")\n",
    "    print(f\"Total URLs: {len(url_df)}\")\n",
    "    print(f\"Unique users: {url_df['user_id'].nunique()}\")\n",
    "    print(f\"Unique domains: {url_df['domain'].nunique()}\")\n",
    "    \n",
    "    display(url_df.head(10))\n",
    "\n",
    "\n",
    "\n",
    "# Fixed Domain Analysis with proper formatting\n",
    "if 'url_df' in locals() and url_df is not None:\n",
    "    print(\"üîç DOMAIN ANALYSIS (FIXED)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group by domain and calculate statistics\n",
    "    domain_stats = url_df.groupby('domain').agg({\n",
    "        'user_id': 'nunique',  # Count unique users\n",
    "        'full_url': 'count',   # Count total URLs\n",
    "        'event_count': 'sum'   # Sum event counts\n",
    "    }).rename(columns={\n",
    "        'user_id': 'unique_users',\n",
    "        'full_url': 'total_urls',\n",
    "        'event_count': 'total_events'\n",
    "    })\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_urls = len(url_df)\n",
    "    total_users = url_df['user_id'].nunique()\n",
    "    \n",
    "    domain_stats['url_percentage'] = (domain_stats['total_urls'] / total_urls * 100).round(1)\n",
    "    domain_stats['user_percentage'] = (domain_stats['unique_users'] / total_users * 100).round(1)\n",
    "    \n",
    "    # Sort by total URLs (most frequent first)\n",
    "    domain_stats = domain_stats.sort_values('total_urls', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"Total URLs processed: {total_urls:,}\")\n",
    "    print(f\"Total unique users: {total_users:,}\")\n",
    "    print(f\"Unique domains found: {len(domain_stats):,}\")\n",
    "    \n",
    "    print(f\"\\nüîù Top Domains by URL Frequency:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Rank':<4} {'Domain':<45} {'URLs':<8} {'Users':<8} {'Events':<10} {'URL %':<8} {'User %'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Fixed formatting - convert to int before using :d format\n",
    "    for i, (domain, stats) in enumerate(domain_stats.head(20).iterrows(), 1):\n",
    "        total_urls_int = int(stats['total_urls'])\n",
    "        unique_users_int = int(stats['unique_users']) \n",
    "        total_events_int = int(stats['total_events'])\n",
    "        print(f\"{i:3d}. {domain:<45} {total_urls_int:6d}   {unique_users_int:6d}   {total_events_int:8d}   {stats['url_percentage']:5.1f}%   {stats['user_percentage']:5.1f}%\")\n",
    "    \n",
    "    # Store the results for further analysis\n",
    "    domain_analysis_df = domain_stats.reset_index()\n",
    "    \n",
    "    print(f\"\\nüíæ Domain analysis saved to 'domain_analysis_df' variable\")\n",
    "    display(domain_analysis_df.head(10))\n",
    "\n",
    "\n",
    "# Event Fetching Pipeline - Get detailed events from events collection\n",
    "def fetch_user_events(user_ids, db_name=\"test\", collection_name=\"events\", limit_per_user=None, min_events=1000):\n",
    "    \"\"\"\n",
    "    Fetch detailed events for specific users from the events collection\n",
    "    \n",
    "    Args:\n",
    "        user_ids: List of user IDs to fetch events for\n",
    "        db_name: Database name\n",
    "        collection_name: Events collection name\n",
    "        limit_per_user: Maximum events per user (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all events for the specified users\n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    client = connect_to_mongodb()\n",
    "    if not client:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        print(f\"Fetching events for {len(user_ids)} users from {db_name}.{collection_name}\")\n",
    "        \n",
    "        # Build query for multiple users\n",
    "        query = {\"userId\": {\"$in\": user_ids}}\n",
    "        \n",
    "        # Fetch events\n",
    "        events = []\n",
    "        \n",
    "        if limit_per_user:\n",
    "            # Fetch limited events for each user individually\n",
    "            for user_id in user_ids:\n",
    "                user_query = {\"userId\": user_id}\n",
    "                user_events = list(collection.find(user_query)\n",
    "                                #  .sort(\"timestamp\", 1)  # Sort by timestamp ascending\n",
    "                                 .limit(limit_per_user))\n",
    "                if len(user_events) > min_events:\n",
    "                    events.extend(user_events)\n",
    "                else:\n",
    "                    print(f\"No events found for user {user_id}\")\n",
    "                \n",
    "            print(f\"Fetched up to {limit_per_user} events per user\")\n",
    "        else:\n",
    "            # Fetch all events for all users\n",
    "            events = list(collection.find(query))\n",
    "        \n",
    "        print(f\"Found {len(events)} events\")\n",
    "        \n",
    "        if not events:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        events_df = pd.DataFrame(events)\n",
    "        \n",
    "        # Convert timestamp to numeric for analysis\n",
    "        if 'timestamp' in events_df.columns:\n",
    "            events_df['timestamp_num'] = pd.to_numeric(events_df['timestamp'], errors='coerce')\n",
    "            events_df['timestamp_dt'] = pd.to_datetime(events_df['timestamp_num'], unit='ms', errors='coerce')\n",
    "        \n",
    "        # Sort by user and timestamp\n",
    "        events_df = events_df.sort_values(['userId', 'timestamp_num'])\n",
    "        \n",
    "        return events_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching events: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n",
    "\n",
    "# User Filtering Pipeline - Extract userIds based on domain/URL criteria\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def break_into_sessions(user_events, session_gap_minutes=5, max_sessions=500):\n",
    "    \"\"\"\n",
    "    Break user events into separate sessions based on time gaps\n",
    "    \n",
    "    Args:\n",
    "        user_events: DataFrame with events for a single user\n",
    "        session_gap_minutes: Minutes of inactivity to consider a new session\n",
    "    \n",
    "    Returns:\n",
    "        List of session DataFrames\n",
    "    \"\"\"\n",
    "    if len(user_events) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    user_events = user_events.sort_values('timestamp_num').copy()\n",
    "    \n",
    "    # Calculate time differences in minutes\n",
    "    user_events['time_diff_minutes'] = user_events['timestamp_num'].diff() / (1000 * 60)\n",
    "    \n",
    "    # Identify session breaks (gaps larger than session_gap_minutes)\n",
    "    session_breaks = user_events['time_diff_minutes'] > session_gap_minutes\n",
    "    \n",
    "    # Create session IDs\n",
    "    user_events['session_id'] = session_breaks.cumsum()\n",
    "    \n",
    "    # Split into separate sessions\n",
    "    sessions = []\n",
    "    for session_id in user_events['session_id'].unique():\n",
    "        session_events = user_events[user_events['session_id'] == session_id].copy()\n",
    "        if len(session_events) >= 2:  # Only include sessions with at least 2 events\n",
    "            sessions.append(session_events)\n",
    "            if len(sessions) >= max_sessions:\n",
    "                print(f\"Found {len(sessions)} sessions\")\n",
    "                break\n",
    "    \n",
    "    return sessions\n",
    "\n",
    "\n",
    "def analyze_event_patterns(events_df, session_gap_minutes=5):\n",
    "    \"\"\"\n",
    "    Analyze event patterns for distinguishing features with session segmentation\n",
    "    \n",
    "    Args:\n",
    "        events_df: DataFrame with user events\n",
    "        session_gap_minutes: Minutes of inactivity to consider a new session\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with pattern analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if events_df is None or len(events_df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Event type distribution\n",
    "    event_types = events_df['eventName'].value_counts()\n",
    "    results['event_types'] = event_types.to_dict()\n",
    "    \n",
    "    # 2. Session-based timing analysis\n",
    "    user_timing = []\n",
    "    total_sessions = 0\n",
    "    \n",
    "    for user_id in events_df['userId'].unique():\n",
    "        user_events = events_df[events_df['userId'] == user_id].copy()\n",
    "        \n",
    "        if len(user_events) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Break user events into sessions\n",
    "        sessions = break_into_sessions(user_events, session_gap_minutes)\n",
    "        \n",
    "        if not sessions:\n",
    "            continue\n",
    "        \n",
    "        total_sessions += len(sessions)\n",
    "        \n",
    "        # Analyze each session separately\n",
    "        user_session_data = []\n",
    "        \n",
    "        for i, session_events in enumerate(sessions):\n",
    "            # Calculate intervals between events within this session\n",
    "            session_events = session_events.sort_values('timestamp_num')\n",
    "            session_events['time_diff'] = session_events['timestamp_num'].diff()\n",
    "            \n",
    "            # Remove first row (NaN) and convert to milliseconds\n",
    "            intervals = session_events['time_diff'].dropna()\n",
    "            \n",
    "            if len(intervals) > 0:\n",
    "                session_duration = (session_events['timestamp_num'].max() - \n",
    "                                  session_events['timestamp_num'].min()) / 1000  # Convert to seconds\n",
    "                \n",
    "                # Calculate consistency metrics\n",
    "                intervals_seconds = intervals / 1000  # Convert to seconds\n",
    "                avg_interval = intervals_seconds.mean()\n",
    "                \n",
    "                # Coefficient of Variation (CV) - normalized measure of variability\n",
    "                cv_interval = (intervals_seconds.std() / avg_interval) if avg_interval > 0 else 0\n",
    "                \n",
    "                # Calculate trend in intervals (are they getting faster/slower?)\n",
    "                if len(intervals_seconds) >= 3:\n",
    "                    # Linear regression slope to detect trend\n",
    "                    x = np.arange(len(intervals_seconds))\n",
    "                    slope = np.polyfit(x, intervals_seconds, 1)[0]\n",
    "                    interval_trend = slope  # Positive = getting slower, Negative = getting faster\n",
    "                    \n",
    "                    # R-squared to measure how linear the trend is\n",
    "                    correlation = np.corrcoef(x, intervals_seconds)[0, 1]\n",
    "                    trend_strength = correlation ** 2 if not np.isnan(correlation) else 0\n",
    "                else:\n",
    "                    interval_trend = 0\n",
    "                    trend_strength = 0\n",
    "                \n",
    "                # Consistency categories\n",
    "                if cv_interval < 0.3:\n",
    "                    consistency_level = \"Very Consistent\"\n",
    "                elif cv_interval < 0.6:\n",
    "                    consistency_level = \"Moderately Consistent\"\n",
    "                elif cv_interval < 1.0:\n",
    "                    consistency_level = \"Variable\"\n",
    "                else:\n",
    "                    consistency_level = \"Highly Variable\"\n",
    "                \n",
    "                # Pattern detection within session\n",
    "                intervals_array = intervals_seconds.values\n",
    "                \n",
    "                # Detect acceleration/deceleration patterns\n",
    "                if len(intervals_array) >= 5:\n",
    "                    first_half_avg = intervals_array[:len(intervals_array)//2].mean()\n",
    "                    second_half_avg = intervals_array[len(intervals_array)//2:].mean()\n",
    "                    pace_change = (second_half_avg - first_half_avg) / first_half_avg * 100\n",
    "                else:\n",
    "                    pace_change = 0\n",
    "                \n",
    "                session_data = {\n",
    "                    'user_id': user_id,\n",
    "                    'session_number': i + 1,\n",
    "                    'total_events': len(session_events),\n",
    "                    'session_duration': session_duration,\n",
    "                    'avg_interval': avg_interval,\n",
    "                    'median_interval': intervals_seconds.median(),\n",
    "                    'min_interval': intervals_seconds.min(),\n",
    "                    'max_interval': intervals_seconds.max(),\n",
    "                    'std_interval': intervals_seconds.std(),\n",
    "                    \n",
    "                    # NEW: Consistency metrics\n",
    "                    'cv_interval': cv_interval,  # Coefficient of variation\n",
    "                    'consistency_level': consistency_level,\n",
    "                    'interval_trend': interval_trend,  # Slope of interval change\n",
    "                    'trend_strength': trend_strength,  # How linear the trend is (0-1)\n",
    "                    'pace_change_percent': pace_change,  # % change from first to second half\n",
    "                    \n",
    "                    'event_types': session_events['eventName'].value_counts().to_dict(),\n",
    "                    'start_time': session_events['timestamp_dt'].min(),\n",
    "                    'end_time': session_events['timestamp_dt'].max()\n",
    "                }\n",
    "                \n",
    "                user_session_data.append(session_data)\n",
    "        \n",
    "        # Add all sessions for this user\n",
    "        user_timing.extend(user_session_data)\n",
    "    \n",
    "    results['user_timing'] = pd.DataFrame(user_timing)\n",
    "    results['total_sessions'] = total_sessions\n",
    "    results['session_gap_minutes'] = session_gap_minutes\n",
    "    \n",
    "    print(f\"    üìä Broke {len(events_df['userId'].unique())} users into {total_sessions} sessions\")\n",
    "    print(f\"    ‚è±Ô∏è Session gap threshold: {session_gap_minutes} minutes\")\n",
    "    \n",
    "    # 3. Common event sequences (within sessions)\n",
    "    sequences = []\n",
    "    for user_id in events_df['userId'].unique():\n",
    "        user_events = events_df[events_df['userId'] == user_id].copy()\n",
    "        \n",
    "        # Break into sessions first\n",
    "        sessions = break_into_sessions(user_events, session_gap_minutes)\n",
    "        \n",
    "        # Extract sequences from each session separately\n",
    "        for session_events in sessions:\n",
    "            session_events = session_events.sort_values('timestamp_num')\n",
    "            event_sequence = session_events['eventName'].tolist()\n",
    "            \n",
    "            # Create 3-event sequences within this session\n",
    "            for i in range(len(event_sequence) - 2):\n",
    "                seq = tuple(event_sequence[i:i+3])\n",
    "                sequences.append(seq)\n",
    "    \n",
    "    if sequences:\n",
    "        from collections import Counter\n",
    "        sequence_counts = Counter(sequences)\n",
    "        results['common_sequences'] = dict(sequence_counts.most_common(20))\n",
    "    \n",
    "    # 4. URL patterns\n",
    "    if 'url' in events_df.columns:\n",
    "        url_patterns = events_df.groupby('userId')['url'].nunique()\n",
    "        results['urls_per_user'] = {\n",
    "            'mean': url_patterns.mean(),\n",
    "            'median': url_patterns.median(),\n",
    "            'min': url_patterns.min(),\n",
    "            'max': url_patterns.max()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def filter_users_by_criteria(url_df, domain_filters=None, url_filters=None, min_events=None, max_events=None, max_users=None):\n",
    "    \"\"\"\n",
    "    Filter users based on domain or URL criteria and optional event count thresholds\n",
    "    \n",
    "    Args:\n",
    "        url_df: DataFrame with user URL data\n",
    "        domain_filters: List of domains to include \n",
    "        url_filters: List of full URLs to include\n",
    "        min_events: Minimum number of events per user (optional)\n",
    "        max_events: Maximum number of events per user (optional)\n",
    "        max_users: Maximum number of users to return (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with filtered users and their metadata\n",
    "    \"\"\"\n",
    "    url_df = url_df[url_df['user_id'] != '0']\n",
    "    # Filter by domains or URLs\n",
    "    if domain_filters and url_filters:\n",
    "        # Filter by both domains AND URLs (union)\n",
    "        domain_mask = url_df['domain'].isin(domain_filters)\n",
    "        url_mask = url_df['full_url'].isin(url_filters)\n",
    "        filtered_df = url_df[domain_mask | url_mask].copy()\n",
    "        print(f\"    üîç Filtering by {len(domain_filters)} domains AND {len(url_filters)} specific URLs\")\n",
    "    elif domain_filters:\n",
    "        # Filter by domains only\n",
    "        filtered_df = url_df[url_df['domain'].isin(domain_filters)].copy()\n",
    "        print(f\"    üîç Filtering by {len(domain_filters)} domains\")\n",
    "    elif url_filters:\n",
    "        # Filter by specific URLs only\n",
    "        filtered_df = url_df[url_df['full_url'].isin(url_filters)].copy()\n",
    "        print(f\"    üîç Filtering by {len(url_filters)} specific URLs\")\n",
    "    else:\n",
    "        # No filters - use all data\n",
    "        filtered_df = url_df.copy()\n",
    "        print(f\"    üîç No filters applied - using all data\")\n",
    "    \n",
    "    # Group by user and aggregate\n",
    "    user_summary = filtered_df.groupby('user_id').agg({\n",
    "        'domain': lambda x: list(x.unique()),  # List of domains visited\n",
    "        'full_url': 'count',  # Number of URLs visited\n",
    "        'event_count': 'first',  # Event count from metadata\n",
    "        'first_timestamp': 'first',\n",
    "        'last_timestamp': 'first',\n",
    "        'user_agent': 'first',\n",
    "        'source_ip': 'first'\n",
    "    }).rename(columns={'full_url': 'url_count'})\n",
    "    \n",
    "    # Apply event count filters if specified\n",
    "    if min_events is not None:\n",
    "        user_summary = user_summary[user_summary['event_count'] >= min_events]\n",
    "    \n",
    "    if max_events is not None:\n",
    "        user_summary = user_summary[user_summary['event_count'] <= max_events]\n",
    "    \n",
    "    # Calculate session duration\n",
    "    user_summary['session_duration'] = 0\n",
    "    for idx, row in user_summary.iterrows():\n",
    "        if row['first_timestamp'] and row['last_timestamp']:\n",
    "            try:\n",
    "                first_ts = int(row['first_timestamp'])\n",
    "                last_ts = int(row['last_timestamp'])\n",
    "                \n",
    "                user_summary.loc[idx, 'session_duration'] = (last_ts - first_ts) / 1000\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # Apply user limit if specified\n",
    "    result_df = user_summary.reset_index()\n",
    "    if max_users is not None and len(result_df) > max_users:\n",
    "        # Sort by event count (descending) to get most active users first\n",
    "        result_df = result_df.sort_values('event_count', ascending=False).head(max_users)\n",
    "        print(f\"    ‚ö†Ô∏è Limited to top {max_users} users (sorted by event count)\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage - filter users by specific domains\n",
    "if 'url_df' in locals():\n",
    "    print(\"üéØ USER FILTERING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define advanced domain groups with URL-specific filtering\n",
    "    advanced_domain_groups = {\n",
    "        'tax1_users': {\n",
    "            'url_filters': [\n",
    "                # 'https://PLACEHOLDER_TAX1.com'\n",
    "            ],\n",
    "            # 'domain_filters': [''],\n",
    "            'description': 'All Tax1 domain users'\n",
    "        },\n",
    "        'tax2_users': {\n",
    "            'url_filters': [\n",
    "                ''\n",
    "            ],\n",
    "            # 'domain_filters': ['https://PLACEHOLDER_TAX2.com'],\n",
    "            'description': 'All Tax2 domain users'\n",
    "        },\n",
    "        'profilic_survey_users': {\n",
    "            'url_filters': [\n",
    "                #'https://PLACEHOLDER_SURVEY.com'\n",
    "            ],\n",
    "            'description': 'Profilic Survey users'\n",
    "        },\n",
    "        'browser_use_agents': {\n",
    "            'url_filters': [\n",
    "                'https://AGENT_SURVVEY_PLACEHOLDER.com/?agent=browser-use'\n",
    "            ],\n",
    "            'description': 'Browser Use Agents'\n",
    "        },\n",
    "        'skyvern_agents': {\n",
    "            'url_filters': [\n",
    "                'https://AGENT_SURVVEY_PLACEHOLDER.com/?agent=skyvern'\n",
    "            ],\n",
    "            'description': 'Skyvern Agents'\n",
    "        },\n",
    "    }\n",
    "\n",
    "    max_users = {\n",
    "        # 'tax1_users': 20,\n",
    "        # 'tax2_users': 20,\n",
    "        # 'profilic_survey_users': 20,\n",
    "        # 'browser_use_agents': 20,\n",
    "        # 'skyvern_agents': 20,\n",
    "    }\n",
    "    \n",
    "    # Filter users for each advanced group\n",
    "    user_groups = {}\n",
    "    \n",
    "    for group_name, criteria in advanced_domain_groups.items():\n",
    "        print(f\"\\nüéØ Processing {group_name.upper()}: {criteria['description']}\")\n",
    "        \n",
    "        # Extract filtering parameters\n",
    "        domain_filters = criteria.get('domain_filters', None)\n",
    "        url_filters = criteria.get('url_filters', None)\n",
    "        \n",
    "        # Skip if no valid URLs found (e.g., for tax_2023_users if no matches)\n",
    "        if url_filters is not None and len(url_filters) == 0:\n",
    "            print(f\"    ‚ö†Ô∏è No matching URLs found for {group_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Limit to 30 users per group, minimum 10 events, get most active users\n",
    "        filtered_users = filter_users_by_criteria(\n",
    "            url_df, \n",
    "            domain_filters=domain_filters,\n",
    "            url_filters=url_filters,\n",
    "            min_events=10, \n",
    "            max_users=max_users.get(group_name, None)\n",
    "        )\n",
    "        \n",
    "        user_groups[group_name] = filtered_users\n",
    "        \n",
    "        print(f\"üìä Results for {group_name.upper()}:\")\n",
    "        print(f\"  ‚Ä¢ Total users: {len(filtered_users):,}\")\n",
    "        if len(filtered_users) > 0:\n",
    "            print(f\"  ‚Ä¢ Avg events per user: {filtered_users['event_count'].mean():.1f}\")\n",
    "            print(f\"  ‚Ä¢ Avg session duration: {filtered_users['session_duration'].mean():.1f}s\")\n",
    "            print(f\"  ‚Ä¢ Sample user IDs: {list(filtered_users['user_id'].head(3))}\")\n",
    "            \n",
    "            # Show some sample URLs for URL-filtered groups\n",
    "            if url_filters and len(url_filters) > 0:\n",
    "                print(f\"  ‚Ä¢ Sample URLs matched: {url_filters[:3]}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No users found matching criteria\")\n",
    "    \n",
    "    print(f\"\\nüíæ User groups saved to 'user_groups' dictionary\")\n",
    "\n",
    "# File-based User Group Loading\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_file_based_user_groups(base_directory, file_extension=\".json\"):\n",
    "    \"\"\"\n",
    "    Load user groups from directory structure where each subdirectory represents a group\n",
    "    \n",
    "    Args:\n",
    "        base_directory: Base path containing group subdirectories (e.g., \"forensics_data/\")\n",
    "        file_extension: File extension to look for (default: \".json\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with group names and user IDs from files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üìÅ LOADING FILE-BASED USER GROUPS\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Base directory: {base_directory}\")\n",
    "    \n",
    "    file_user_groups = {}\n",
    "    \n",
    "    if not os.path.exists(base_directory):\n",
    "        print(f\"‚ö†Ô∏è Directory {base_directory} does not exist\")\n",
    "        return file_user_groups\n",
    "    \n",
    "    # Get all subdirectories\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "               if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    print(f\"Found {len(subdirs)} group directories: {subdirs}\")\n",
    "    \n",
    "    for group_dir in subdirs:\n",
    "        group_path = os.path.join(base_directory, group_dir)\n",
    "        \n",
    "        # Find all JSON files in this directory\n",
    "        json_pattern = os.path.join(group_path, f\"*{file_extension}\")\n",
    "        json_files = glob.glob(json_pattern)\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"‚ö†Ô∏è No {file_extension} files found in {group_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract user IDs from filenames\n",
    "        user_ids = []\n",
    "        valid_files = []\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            filename = os.path.basename(json_file)\n",
    "            # Remove extension to get user ID\n",
    "            user_id = filename.replace(file_extension, \"\")\n",
    "            \n",
    "            # Validate that file is readable\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, dict) and 'trace' in data:\n",
    "                        user_ids.append(user_id)\n",
    "                        valid_files.append(json_file)\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid file {filename}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if user_ids:\n",
    "            file_user_groups[group_dir] = {\n",
    "                'user_ids': user_ids,\n",
    "                'file_paths': valid_files,\n",
    "                'source': 'file',\n",
    "                'description': f'File-based group from {group_dir} directory',\n",
    "                'count': len(user_ids)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüìä {group_dir.upper()}:\")\n",
    "            print(f\"  ‚Ä¢ Total users: {len(user_ids)}\")\n",
    "            print(f\"  ‚Ä¢ Sample user IDs: {user_ids[:3]}\")\n",
    "            print(f\"  ‚Ä¢ Source: File-based ({group_path})\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No valid users found in {group_dir}\")\n",
    "    \n",
    "    return file_user_groups\n",
    "\n",
    "def load_events_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load events from a JSON file with forensics data structure\n",
    "    \n",
    "    Expected format:\n",
    "    {\n",
    "        \"user_id\": \"0a5f288e-5220-4c83-aa3b-976a9bbca2d1\",\n",
    "        \"device\": \"desktop\", \n",
    "        \"trace\": [\n",
    "            {\"event_name\": \"mousemove\", \"timestamp\": \"1667342141173\", \"position\": {\"x\": 1298, \"y\": 257}},\n",
    "            {\"event_name\": \"mousedown\", \"timestamp\": \"1667342141555\", \"position\": {\"x\": 1300, \"y\": 257}},\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with events in standardized format\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle forensics data structure\n",
    "        if isinstance(data, dict) and 'trace' in data:\n",
    "            # Extract metadata\n",
    "            user_id = data.get('user_id', 'unknown')\n",
    "            device = data.get('device', 'unknown')\n",
    "            \n",
    "            # Get events from trace\n",
    "            events = data['trace']\n",
    "            \n",
    "            if not events:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            events_df = pd.DataFrame(events)\n",
    "            \n",
    "            # Standardize column names to match MongoDB format\n",
    "            column_mapping = {\n",
    "                'event_name': 'eventName',\n",
    "                'timestamp': 'timestamp'\n",
    "            }\n",
    "            \n",
    "            events_df = events_df.rename(columns=column_mapping)\n",
    "            \n",
    "            # Add user metadata\n",
    "            events_df['userId'] = user_id\n",
    "            events_df['device'] = device\n",
    "            \n",
    "            # Handle position data - convert to pos format like MongoDB\n",
    "            if 'position' in events_df.columns:\n",
    "                events_df['pos'] = events_df['position'].apply(\n",
    "                    lambda x: json.dumps(x) if isinstance(x, dict) else '{\"x\":0,\"y\":0}'\n",
    "                )\n",
    "            \n",
    "            # Ensure timestamp is numeric\n",
    "            if 'timestamp' in events_df.columns:\n",
    "                events_df['timestamp_num'] = pd.to_numeric(events_df['timestamp'], errors='coerce')\n",
    "                events_df['timestamp_dt'] = pd.to_datetime(events_df['timestamp_num'], unit='ms', errors='coerce')\n",
    "            \n",
    "            # Add missing columns with default values to match MongoDB structure\n",
    "            default_columns = {\n",
    "                'cursor': 0,\n",
    "                'element': '/',\n",
    "                'attrs': '',\n",
    "                'extra': '{\"trusted\":true}',\n",
    "                'url': f'file:///{device}_forensics_data',\n",
    "                'dimensions': '{\"screenw\":1920,\"screenh\":1080,\"winw\":1920,\"winh\":945,\"docw\":1903,\"doch\":2542}',\n",
    "                'fingerprint': f'{{\"source\":\"forensics_data\",\"userAgent\":\"ForensicsData/{device}\"}}'\n",
    "            }\n",
    "            \n",
    "            for col, default_val in default_columns.items():\n",
    "                if col not in events_df.columns:\n",
    "                    events_df[col] = default_val\n",
    "            \n",
    "            return events_df\n",
    "            \n",
    "        # Fallback for other formats\n",
    "        elif isinstance(data, list):\n",
    "            # Direct list of events\n",
    "            events = data\n",
    "        elif isinstance(data, dict):\n",
    "            if 'events' in data:\n",
    "                # Events nested under 'events' key\n",
    "                events = data['events']\n",
    "            else:\n",
    "                # Assume the dict itself represents events\n",
    "                events = [data]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame (fallback path)\n",
    "        events_df = pd.DataFrame(events)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if 'timestamp' in events_df.columns:\n",
    "            events_df['timestamp_num'] = pd.to_numeric(events_df['timestamp'], errors='coerce')\n",
    "            events_df['timestamp_dt'] = pd.to_datetime(events_df['timestamp_num'], unit='ms', errors='coerce')\n",
    "        \n",
    "        return events_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def show_user_group_statistics(mongo_user_groups, file_user_groups):\n",
    "    \"\"\"\n",
    "    Display detailed statistics about available users in each group\n",
    "    \"\"\"\n",
    "    print(f\"üìä USER GROUP STATISTICS\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    total_mongo_users = 0\n",
    "    total_file_users = 0\n",
    "    \n",
    "    if mongo_user_groups:\n",
    "        print(f\"\\nüóÑÔ∏è MongoDB Groups:\")\n",
    "        for group_name, users_df in mongo_user_groups.items():\n",
    "            user_count = len(users_df)\n",
    "            total_mongo_users += user_count\n",
    "            avg_events = users_df['event_count'].mean() if 'event_count' in users_df.columns else 0\n",
    "            print(f\"  ‚Ä¢ {group_name:<25} {user_count:>6} users (avg {avg_events:.0f} events)\")\n",
    "    \n",
    "    if file_user_groups:\n",
    "        print(f\"\\nüìÅ File-based Groups:\")\n",
    "        for group_name, group_info in file_user_groups.items():\n",
    "            user_count = group_info['count']\n",
    "            total_file_users += user_count\n",
    "            print(f\"  ‚Ä¢ {group_name:<25} {user_count:>6} users (from files)\")\n",
    "    \n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total MongoDB users: {total_mongo_users:,}\")\n",
    "    print(f\"  ‚Ä¢ Total file-based users: {total_file_users:,}\")\n",
    "    print(f\"  ‚Ä¢ Grand total: {total_mongo_users + total_file_users:,} users\")\n",
    "\n",
    "def fetch_events_by_url(url, db_name=\"test\", collection_name=\"events\", limit_per_user=None, min_events=1000):\n",
    "    \"\"\"\n",
    "    Fetch detailed events for specific users from the events collection\n",
    "    \n",
    "    Args:\n",
    "        url: List of user IDs to fetch events for\n",
    "        db_name: Database name\n",
    "        collection_name: Events collection name\n",
    "        limit_per_user: Maximum events per user (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all events for the specified users\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = connect_to_mongodb()\n",
    "    if not client:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        print(f\"Fetching events for {len(url)} url from {db_name}.{collection_name}\")\n",
    "\n",
    "        query = {\"url\": url}\n",
    "        \n",
    "        # Build query for multiple users\n",
    "        events = list(collection.find(query))\n",
    "        \n",
    "        print(f\"Found {len(events)} events\")\n",
    "        \n",
    "        if not events:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        events_df = pd.DataFrame(events)\n",
    "        \n",
    "        # Convert timestamp to numeric for analysis\n",
    "        if 'timestamp' in events_df.columns:\n",
    "            events_df['timestamp_num'] = pd.to_numeric(events_df['timestamp'], errors='coerce')\n",
    "            events_df['timestamp_dt'] = pd.to_datetime(events_df['timestamp_num'], unit='ms', errors='coerce')\n",
    "        \n",
    "        # Sort by user and timestamp\n",
    "        events_df = events_df.sort_values(['userId', 'timestamp_num'])\n",
    "        \n",
    "        return events_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching events: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n",
    "def create_mixed_sample_events(mongo_user_groups, file_user_groups, max_users_per_group=100):\n",
    "    \"\"\"\n",
    "    Create sample events from both MongoDB and file-based sources\n",
    "    \n",
    "    Args:\n",
    "        mongo_user_groups: User groups from MongoDB (existing format)\n",
    "        file_user_groups: User groups from files\n",
    "        max_users_per_group: Maximum users to process per group\n",
    "    \n",
    "    Returns:\n",
    "        Combined sample_events dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ CREATING MIXED SAMPLE EVENTS\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Max users per group: {max_users_per_group}\")\n",
    "    \n",
    "    # Show statistics first\n",
    "    show_user_group_statistics(mongo_user_groups, file_user_groups)\n",
    "    \n",
    "    mixed_sample_events = {}\n",
    "    \n",
    "    # Process MongoDB groups (existing logic)\n",
    "    for group_name, users_df in mongo_user_groups.items():\n",
    "        if len(users_df) > 0:\n",
    "            sample_size = min(max_users_per_group, len(users_df))\n",
    "            sample_users = users_df.head(sample_size)['user_id'].tolist()\n",
    "            \n",
    "            print(f\"\\nüéØ Fetching MongoDB events for {group_name} ({sample_size} users)...\")\n",
    "\n",
    "            # Fetch events from MongoDB\n",
    "            if group_name == 'open_operator_agents':\n",
    "                events_df = fetch_events_by_url(url='https://agents.cheapernyhomes.com/?agent=openoperator')\n",
    "            # Fetch events from MongoDB\n",
    "            else:\n",
    "                events_df = fetch_user_events(sample_users, limit_per_user=10000)\n",
    "\n",
    "            \n",
    "            \n",
    "            if events_df is not None and len(events_df) > 0:\n",
    "                patterns = analyze_event_patterns(events_df)\n",
    "                \n",
    "                mixed_sample_events[group_name] = {\n",
    "                    'events_df': events_df,\n",
    "                    'patterns': patterns,\n",
    "                    'user_count': sample_size,\n",
    "                    'total_events': len(events_df),\n",
    "                    'source': 'mongodb'\n",
    "                }\n",
    "                \n",
    "                print(f\"  ‚úì Found {len(events_df):,} events from MongoDB\")\n",
    "    \n",
    "    # Process file-based groups\n",
    "    for group_name, group_info in file_user_groups.items():\n",
    "        user_ids = group_info['user_ids']\n",
    "        file_paths = group_info['file_paths']\n",
    "        \n",
    "        sample_size = min(max_users_per_group, len(user_ids))\n",
    "        sample_users = user_ids[:sample_size]\n",
    "        sample_files = file_paths[:sample_size]\n",
    "        \n",
    "        print(f\"\\nüéØ Loading file events for {group_name} ({sample_size} users)...\")\n",
    "        \n",
    "        # Load events from files\n",
    "        all_events = []\n",
    "        successful_loads = 0\n",
    "        \n",
    "        for i, (user_id, file_path) in enumerate(zip(sample_users, sample_files)):\n",
    "            # Show progress for larger datasets\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(f\"    üìà Processed {i}/{len(sample_users)} files...\")\n",
    "\n",
    "            events_df = load_events_from_file(file_path)\n",
    "            \n",
    "            if not events_df.empty:\n",
    "                # Add user ID if not present\n",
    "                if 'userId' not in events_df.columns:\n",
    "                    events_df['userId'] = user_id\n",
    "                \n",
    "                all_events.append(events_df)\n",
    "                successful_loads += 1\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è No events loaded from {os.path.basename(file_path)}\")\n",
    "        \n",
    "        if all_events:\n",
    "            # Combine all events\n",
    "            combined_events_df = pd.concat(all_events, ignore_index=True)\n",
    "            \n",
    "            # Analyze patterns\n",
    "            patterns = analyze_event_patterns(combined_events_df)\n",
    "            \n",
    "            mixed_sample_events[group_name] = {\n",
    "                'events_df': combined_events_df,\n",
    "                'patterns': patterns,\n",
    "                'user_count': successful_loads,\n",
    "                'total_events': len(combined_events_df),\n",
    "                'source': 'file'\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úì Found {len(combined_events_df):,} events from {successful_loads} files\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No valid events found for {group_name}\")\n",
    "    \n",
    "    print(f\"\\nüìä Mixed Sample Events Summary:\")\n",
    "    for group_name, data in mixed_sample_events.items():\n",
    "        source = data.get('source', 'unknown')\n",
    "        print(f\"  ‚Ä¢ {group_name}: {data['user_count']} users, {data['total_events']:,} events ({source})\")\n",
    "    \n",
    "    return mixed_sample_events\n",
    "\n",
    "# # Example usage - load file-based groups\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"üìÅ FILE-BASED USER GROUP INTEGRATION\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# Load file-based user groups\n",
    "base_dir = \"forensics_data\"  # Change this to your actual path\n",
    "file_user_groups = load_file_based_user_groups(base_dir)\n",
    "\n",
    "# Combine with existing MongoDB groups if available\n",
    "if 'user_groups' in locals() and file_user_groups:\n",
    "    print(f\"\\nüîÑ Combining MongoDB and file-based groups...\")\n",
    "    \n",
    "    # Create mixed sample events with more users\n",
    "    mixed_sample_events = create_mixed_sample_events(user_groups, file_user_groups, max_users_per_group=100)\n",
    "    \n",
    "    print(f\"\\nüíæ Mixed sample events saved to 'mixed_sample_events' variable\")\n",
    "    print(f\"üìã Available groups: {list(mixed_sample_events.keys())}\")\n",
    "    \n",
    "elif file_user_groups:\n",
    "    print(f\"\\nüìÅ File-based groups loaded:\")\n",
    "    for group_name, info in file_user_groups.items():\n",
    "        print(f\"  ‚Ä¢ {group_name}: {info['count']} users\")\n",
    "    \n",
    "    # Create sample events from files only with more users\n",
    "    mixed_sample_events = create_mixed_sample_events({}, file_user_groups, max_users_per_group=100)\n",
    "    \n",
    "    print(f\"\\nüíæ File-based sample events saved to 'mixed_sample_events' variable\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No file-based groups found. Check the base directory path.\")\n",
    "    print(f\"Expected structure:\")\n",
    "    print(f\"  forensics_data/\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ gremlins/\")\n",
    "    print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ user1.json\")\n",
    "    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ user2.json\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ other_group/\")\n",
    "    print(f\"      ‚îú‚îÄ‚îÄ user3.json\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ user4.json\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
